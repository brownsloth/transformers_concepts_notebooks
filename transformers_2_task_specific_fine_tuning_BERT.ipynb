{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPVxudvqAVhTDsvDZDHwvlr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/brownsloth/transformers_concepts_notebooks/blob/main/transformers_2_task_specific_fine_tuning_BERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "We can add a Head which is compatible with the pre-trained transformer on top of it.\n",
        "Ex: BERT can help us:\n",
        "  - classify sentences by adding appropriate head on pooled output of the final layer\n",
        "  - in question answering task by adding appropriate head on the hidden layer output of the final layer\n",
        "\n",
        "It cannot help us in seq-to-seq tasks like language translation no matter the head we choose.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "Y7glUvZ-zAIY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "collapsed": true,
        "id": "YtKiis6WReHX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install evaluate"
      ],
      "metadata": {
        "id": "VSNlCUVEswEK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datasets import load_dataset, Dataset, DatasetDict\n",
        "from evaluate import load as load_metric\n",
        "from transformers import DataCollatorWithPadding, AutoModelForSequenceClassification,  Trainer, TrainingArguments, AutoTokenizer, AutoModel, AutoConfig\n",
        "from transformers.modeling_outputs import TokenClassifierOutput\n",
        "from transformers import get_scheduler\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.optim import AdamW\n",
        "from torch.utils.data import DataLoader\n",
        "import os\n",
        "from tqdm.auto import tqdm"
      ],
      "metadata": {
        "id": "MxiKm27B2uJ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Prepare dataset"
      ],
      "metadata": {
        "id": "1UuZjPgXXI6U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install -q kaggle\n",
        "from google.colab import files\n",
        "files.upload()\n",
        "\n",
        "#Choose the kaggle.json file that you downloaded\n",
        "!mkdir ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "#Make directory named kaggle and copy kaggle.json file there.\n",
        "! chmod 600 ~/.kaggle/kaggle.json\n",
        "#Change the permissions of the file.\n",
        "! kaggle datasets list"
      ],
      "metadata": {
        "collapsed": true,
        "id": "QbKKPdsvRpOH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download 'rmisra/news-headlines-dataset-for-sarcasm-detection'\n",
        "! mkdir train\n",
        "! unzip news-headlines-dataset-for-sarcasm-detection.zip -d train"
      ],
      "metadata": {
        "collapsed": true,
        "id": "fiXt5erKVGGd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cat train/Sarcasm_Headlines_Dataset_v2.json | head -5"
      ],
      "metadata": {
        "id": "IRCZTW2zWlYD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_path = 'train/Sarcasm_Headlines_Dataset_v2.json'\n",
        "df = pd.read_json(train_path, lines=True)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "vIHPSiIGDIjJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_hf = load_dataset(\"json\", data_files=train_path)\n",
        "print(dataset_hf)"
      ],
      "metadata": {
        "id": "TjNaTcLCWsig"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Convert to pandas for some pre-processing on the daatset\n",
        "dataset_hf.set_format('pandas') #on iteration, rows will be returned in that format\n",
        "dataset_df = dataset_hf['train'][:] #this returns all rows in df format .. basically converting to df\n",
        "dataset_df.head()"
      ],
      "metadata": {
        "id": "DvG5SZP-X1rX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_df.drop(['article_link'], axis=1, inplace=True, errors='ignore')\n",
        "dataset_df.drop_duplicates(subset=['headline'], inplace=True)\n",
        "dataset_df.reset_index() #reset index after droppping duplicates\n",
        "dataset_df = dataset_df[['headline', 'is_sarcastic']]\n",
        "dataset_df.rename(columns={'headline': 'input', 'is_sarcastic': 'label'}, inplace=True)\n",
        "dataset_df.head()"
      ],
      "metadata": {
        "id": "6uA7AcKbaa0v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## convert back to HF dataset\n",
        "dataset_hf = Dataset.from_pandas(dataset_df, preserve_index=False)\n",
        "\n",
        "## Split into train-val-test\n",
        "train_testval_split = dataset_hf.train_test_split(test_size=0.2, seed=42)\n",
        "\n",
        "test_val_split = train_testval_split['test'].train_test_split(test_size=0.5, seed=42)\n",
        "\n",
        "dataset_hf_with_splits = DatasetDict({\n",
        "    'train': train_testval_split['train'],\n",
        "    'valid': test_val_split['train'],\n",
        "    'test': test_val_split['test']\n",
        "})\n",
        "print(dataset_hf_with_splits)"
      ],
      "metadata": {
        "id": "4xDCkrbNbHAx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Prepare the dataset for the specific model"
      ],
      "metadata": {
        "id": "k-em9r16dzvx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint = 'distilbert-base-uncased'\n",
        "# Can use AutoModel to use any model as long as its recognizable from the checkpoint\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "model = AutoModel.from_pretrained(checkpoint)\n",
        "print(tokenizer)\n",
        "print(tokenizer.model_max_length)\n",
        "print(model)"
      ],
      "metadata": {
        "id": "Xdxvox9-dsCg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#can see whats inside the model\n",
        "model.transformer.layer[0].attention"
      ],
      "metadata": {
        "id": "-18vAyCrer86"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## First step: Tokenize the HF dataset using custom tokenizer\n",
        "## Here the tokenizer is based on the pretraiend model\n",
        "## map() method helps apply custom functions fast on the HF dataset\n",
        "\n",
        "def tokenize(batch):\n",
        "  #tokenize all (train test val)\n",
        "  return tokenizer(batch['input'], truncation=True, max_length=tokenizer.model_max_length)\n",
        "\n",
        "tokenized_dataset = dataset_hf_with_splits.map(tokenize, batched=True)\n",
        "print(tokenized_dataset)\n",
        "\n",
        "tokenized_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
        "data_collator  = DataCollatorWithPadding(tokenizer=tokenizer) # just needs to know about the tokenizer that was used"
      ],
      "metadata": {
        "id": "tHCzTsrYfJqB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Write class for the model load and forward pass"
      ],
      "metadata": {
        "id": "PQZpsZmLhl1d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PRETRAINED_OUTPUT_DIM = model.embeddings.word_embeddings.embedding_dim ## assuming the output would be this from the final layer"
      ],
      "metadata": {
        "collapsed": true,
        "id": "6aewpwChjZ3B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SarcasmDetector(nn.Module):\n",
        "  def __init__(self, checkpoint, num_labels):\n",
        "    super(SarcasmDetector, self).__init__()\n",
        "    self.num_labels = num_labels\n",
        "    self.model = AutoModel.from_pretrained(checkpoint, config=AutoConfig.from_pretrained(checkpoint,\n",
        "                                                                                         output_attention=True,\n",
        "                                                                                         output_hidden_state=True))\n",
        "\n",
        "    ## Define New layers here\n",
        "    self.dropout = nn.Dropout(0.1)\n",
        "    self.classifier = nn.Linear(PRETRAINED_OUTPUT_DIM, num_labels)\n",
        "\n",
        "  def forward(self, input_ids=None, attention_mask=None, labels=None):\n",
        "    outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "\n",
        "    last_hidden_state = outputs[0] #0th will give us last hidden state (tokeniwise embeddings), 1st will give us CLS embedding\n",
        "    ## Apply new layers here\n",
        "    sequence_outputs = self.dropout(last_hidden_state)\n",
        "    print(sequence_outputs.shape)\n",
        "    logits = self.classifier(sequence_outputs[:,0,:].view(-1, PRETRAINED_OUTPUT_DIM))\n",
        "\n",
        "    loss = None\n",
        "    if labels is not None:\n",
        "      loss_func = nn.CrossEntropyLoss()\n",
        "      loss = loss_func(logits.view(-1, self.num_labels), labels.view(-1))\n",
        "\n",
        "    #need to send output as TokenClassifierOutput object with all info so HF knows\n",
        "    return TokenClassifierOutput(loss=loss, logits=logits, hidden_states=outputs.hidden_states, attentions=outputs.attentions)\n"
      ],
      "metadata": {
        "id": "pRdep5eXhfyy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Do other things required for training"
      ],
      "metadata": {
        "id": "5-CQSUfPnJyZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Create dataloader instances\n",
        "\n",
        "train_dl = DataLoader(\n",
        "    tokenized_dataset['train'], shuffle=True, batch_size=32, collate_fn = data_collator\n",
        ")\n",
        "val_dl = DataLoader(\n",
        "    tokenized_dataset['valid'], shuffle=True, collate_fn = data_collator\n",
        ")"
      ],
      "metadata": {
        "id": "GL70hLRrm8EI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available else 'cpu')\n",
        "sarcasm_model = SarcasmDetector(checkpoint, num_labels=2).to(device)"
      ],
      "metadata": {
        "id": "PEKvN5n3ohd4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = AdamW(sarcasm_model.parameters(), lr=5e-5)\n",
        "num_epoch = 3\n",
        "num_training_steps = num_epoch * len(train_dl)\n",
        "print(num_training_steps)\n",
        "lr_scheduler = get_scheduler(\n",
        "    'linear',\n",
        "    optimizer = optimizer,\n",
        "    num_warmup_steps=0,\n",
        "    num_training_steps=num_training_steps\n",
        ")"
      ],
      "metadata": {
        "id": "sZbSCaT8r4Ju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metric = load_metric('f1')"
      ],
      "metadata": {
        "id": "PmY2YyTPsZ92"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Write the training loop"
      ],
      "metadata": {
        "id": "CMi-NJWrtBpE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "progress_bar_train = tqdm(range(num_training_steps))\n",
        "progress_bar_eval = tqdm(range(num_epoch * len(val_dl)))\n",
        "\n",
        "for epoch in range(num_epoch):\n",
        "  sarcasm_model.train()\n",
        "  for batch in train_dl:\n",
        "      batch = {k:v.to(device) for k,v in batch.items()}\n",
        "      outputs = sarcasm_model(**batch)\n",
        "      loss = outputs.loss\n",
        "      loss.backward()\n",
        "\n",
        "      optimizer.step()\n",
        "      lr_scheduler.step()\n",
        "      optimizer.zero_grad()\n",
        "      progress_bar_train.update()\n",
        "  sarcasm_model.eval()\n",
        "  for batch in val_dl:\n",
        "      batch = {k:v.to(device) for k,v in batch.items()}\n",
        "      with torch.no_grad():\n",
        "        outputs = sarcasm_model(**batch)\n",
        "      logits = outputs.logits\n",
        "      predictions = torch.argmax(logits, dim=-1)\n",
        "      metric.add_batch(predictions=predictions, references=batch['labels'])\n",
        "      progress_bar_eval.update()\n",
        "\n",
        "  print(metric.compute())"
      ],
      "metadata": {
        "id": "VGzGHTNss__9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sarcasm_model.eval()\n",
        "\n",
        "test_dl = val_dl = DataLoader(\n",
        "    tokenized_dataset['test'], batch_size=32, collate_fn = data_collator\n",
        ")\n",
        "\n",
        "for batch in test_dl:\n",
        "    batch = {k:v.to(device) for k,v in batch.items()}\n",
        "    with torch.no_grad():\n",
        "      outputs = sarcasm_model(**batch)\n",
        "    logits = outputs.logits\n",
        "    predictions = torch.argmax(logits, dim=-1)\n",
        "    metric.add_batch(predictions=predictions, references=batch['labels'])\n",
        "\n",
        "print(metric.compute())"
      ],
      "metadata": {
        "id": "soU5jG4cxb9h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We achieved f1 score of 93%"
      ],
      "metadata": {
        "id": "CRzuQ2uZy-Tq"
      }
    }
  ]
}