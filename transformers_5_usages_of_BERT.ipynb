{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPGu1ynnhqpnphTBaxnpVul",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/brownsloth/transformers_concepts_notebooks/blob/main/transformers_5_usages_of_BERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dyCVrt0_v4UQ"
      },
      "outputs": [],
      "source": [
        "import transformers\n",
        "from transformers import BertTokenizer, BertForMaskedLM\n",
        "from torch.nn import functional as F\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ckpt = 'bert-base-uncased'\n",
        "tokenizer = BertTokenizer.from_pretrained(ckpt)\n",
        "text = '2025 will be a great year for all of us!'\n",
        "encoding = tokenizer.encode_plus(text, add_special_token=True, truncation=True, padding='max_length',\n",
        "                                 return_attention_task=True, return_tensor='pt')\n",
        "print(encoding)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "AuJV4-7Gw7yV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## The encoding contains the token ids but also other info\n",
        "print(encoding['input_ids'])\n",
        "print(encoding['attention_mask'])"
      ],
      "metadata": {
        "id": "wlsDrlcUyh9M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Application 1: Predcting masked words!"
      ],
      "metadata": {
        "id": "IeCvizgY0tc6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = 'The Opera House in Australia is in '+ tokenizer.mask_token + ' city'\n",
        "input = tokenizer.encode_plus(text, return_tensors='pt')\n",
        "mask_index = input['input_ids'][0].tolist().index(tokenizer.mask_token_id)\n",
        "model = BertForMaskedLM.from_pretrained(ckpt, return_dict=True) # return_dict helps return a ModelOutput class"
      ],
      "metadata": {
        "id": "x2iQUNEbytYW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = model(**input)\n",
        "logits = output.logits\n",
        "print(logits.shape) # (batch_size, seq_len, vocab_size)"
      ],
      "metadata": {
        "id": "9aOhQuro2WJX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "softmax = F.softmax(logits, dim=-1)\n",
        "mask_words = softmax[0, mask_index,:]\n",
        "print(mask_words.shape)\n",
        "print(mask_words)"
      ],
      "metadata": {
        "id": "RCLUHkmD3nYc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get indices\n",
        "# This gives us index in vocab which is same as token id\n",
        "# can use argmax for the best replacement\n",
        "for token_id in torch.topk(mask_words, 10)[1]:\n",
        "  token = tokenizer.decode(token_id)\n",
        "  new_sentence = text.replace(tokenizer.mask_token, token)\n",
        "  print(new_sentence)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "525GRRjz4CLf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Predicting next sentence!"
      ],
      "metadata": {
        "id": "7ZTb1bPU6aw6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertForNextSentencePrediction, BertTokenizer\n",
        "\n",
        "ckpt = 'bert-base-uncased'\n",
        "tokenizer = BertTokenizer.from_pretrained(ckpt)\n",
        "model = BertForNextSentencePrediction.from_pretrained(ckpt)"
      ],
      "metadata": {
        "id": "l3CPEsrA6YcL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"I came back from office in the evening\"\n",
        "possible_next_sentence = \"I opened my beer after office\""
      ],
      "metadata": {
        "id": "cItwkg4y8FX6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input = tokenizer.encode_plus(prompt, possible_next_sentence, return_tensors='pt')\n",
        "outputs = model(**input)\n",
        "print(outputs)"
      ],
      "metadata": {
        "id": "3daCCgru8T20"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "softmax = F.softmax(outputs['logits'], dim=1)\n",
        "print(softmax)"
      ],
      "metadata": {
        "id": "c6EO-Ele8nkx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Question Answering"
      ],
      "metadata": {
        "id": "ZKls6Jab-7p0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertForQuestionAnswering, BertTokenizer\n",
        "\n",
        "ckpt = 'deepset/bert-base-cased-squad2'\n",
        "tokenizer = BertTokenizer.from_pretrained(ckpt)\n",
        "model = BertForQuestionAnswering.from_pretrained(ckpt)"
      ],
      "metadata": {
        "id": "aQxKhfto-92_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "context_text = 'GPT-3 came in 2020'\n",
        "question_text = 'When did GPT-3 come'\n",
        "inputs = tokenizer(question_text, context_text, return_tensors='pt')\n",
        "print(inputs)\n",
        "## its usually taken as input like [CLS] text1 [SEP] text2 [SEP]\n",
        "print(tokenizer.special_tokens_map)\n",
        "print(torch.where(inputs['input_ids'] == tokenizer.cls_token_id)) # 0th is CLS\n",
        "print(torch.where(inputs['input_ids'] == tokenizer.sep_token_id)) # 8th index is SEP for question_text and 16th is SEP for context_text\n",
        "print((inputs['token_type_ids'] == 0).nonzero(as_tuple=True)[0].shape) # from 0 to 8 index is for question_text"
      ],
      "metadata": {
        "id": "WZzU_KgN_jh8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(inputs'].shape)"
      ],
      "metadata": {
        "id": "5WWzdwiACQEt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  outputs = model(**inputs)\n",
        "\n",
        "answer_start_index = outputs.start_logits.argmax()\n",
        "answer_end_index = outputs.end_logits.argmax()\n",
        "print(answer_start_index)\n",
        "print(answer_end_index)\n",
        "## This tells us where within the context does the answer lie?\n",
        "print(tokenizer.decode(inputs.input_ids[0, answer_start_index:answer_end_index+1]))"
      ],
      "metadata": {
        "id": "f9QNEbqwAHab"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}