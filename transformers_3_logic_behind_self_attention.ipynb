{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMpCykgY1JiTm4DmKDQ/GeH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/brownsloth/transformers_concepts_notebooks/blob/main/transformers_3_logic_behind_self_attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "o0-gOkHuBD4v"
      },
      "outputs": [],
      "source": [
        "!pip install bertviz"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from bertviz.transformers_neuron_view import BertModel\n",
        "from transformers import AutoTokenizer, AutoConfig\n",
        "from bertviz.neuron_view import show\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "import torch\n",
        "from math import sqrt"
      ],
      "metadata": {
        "id": "AuJ_lmiKKT-4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_ckpt = 'bert-base-uncased'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
        "model = BertModel.from_pretrained(model_ckpt)\n",
        "text = \"As the aircraft becomes lighter, it flies higher in the air of lower density to maintain the same airspeed.\"\n",
        "\n",
        "show(model, 'bert', tokenizer, text, display_mode='light', layer=0, head=0)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "gnWL-aikBNPd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tokenizer(text, return_tensors='pt', add_special_tokens=False)\n",
        "inputs.input_ids"
      ],
      "metadata": {
        "collapsed": true,
        "id": "2_nQnBe-IcN8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = AutoConfig.from_pretrained(model_ckpt)\n",
        "config"
      ],
      "metadata": {
        "id": "XDKCSIQjKLNf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "token_embeddings_layer = nn.Embedding(config.vocab_size, config.hidden_size) ## this doesnt require more bert context?\n",
        "token_embeddings_layer"
      ],
      "metadata": {
        "id": "fhDhdIynKwh4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_embeddings = token_embeddings_layer(inputs.input_ids)\n",
        "input_embeddings\n",
        "input_embeddings.size()"
      ],
      "metadata": {
        "id": "qj1ASuD1LLSJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Get the Q, K, V vectors and do dot product to calculate attention\n",
        "## All q,k,v are of shape (batch_size, seq_len, hidden_depth)\n",
        "## for understanding purpose we can keep them same for now\n",
        "query = key = value  = input_embeddings\n",
        "\n",
        "dim_k = key.size(-1)\n",
        "attention_scores = torch.bmm(query, key.transpose(1, 2))/sqrt(dim_k) #dimensions 1 and 2 are swapped in key\n",
        "## bmm is required to do batchwise matmul\n",
        "## bmm gets performed only on the last 2 dimensions\n",
        "\n",
        "attention_scores.size()"
      ],
      "metadata": {
        "id": "VPHL6QlqL8hd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "torch.nn.functional in PyTorch is used for operations that do not have trainable parameters or maintain state. It is often imported as F and offers a variety of functions for building neural networks, including:\n",
        "Activation functions: relu, sigmoid, tanh, softmax, etc., are applied element-wise to introduce non-linearity.\n",
        "Convolutional operations: conv2d, conv_transpose2d, etc., perform convolutions for feature extraction.\n",
        "Pooling operations: max_pool2d, avg_pool2d, etc., reduce spatial dimensions.\n",
        "Linear transformations: linear applies a linear transformation.\n",
        "Loss functions: mse_loss, cross_entropy, etc., calculate the difference between predictions and actual values.\n",
        "Dropout: dropout randomly zeroes elements to prevent overfitting.\n",
        "torch.nn.functional is suitable when:\n",
        "You need a simple, stateless operation.\n",
        "You want to define custom operations within a neural network.\n",
        "You need more flexibility than provided by torch.nn modules.\n",
        "In contrast, torch.nn is used for layers with learnable parameters, such as nn.Linear, nn.Conv2d, and nn.BatchNorm2d. These layers manage their weights and biases internally, while torch.nn.functional requires you to handle these parameters manually.\n",
        "\"\"\"\n",
        "attention_weights = F.softmax(attention_scores, dim=1)\n",
        "print(attention_weights.shape)\n",
        "print(attention_weights.sum(dim=-1))\n",
        "print(attention_weights.shape)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "AdlprBRlNQA4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(attention_weights.shape[1]):\n",
        "  print(attention_weights[0][i][i])"
      ],
      "metadata": {
        "id": "gbwNotcnSilm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attention_outputs = torch.bmm(attention_weights, value)\n",
        "\n",
        "print(attention_outputs.shape)\n",
        "## Self attention ~~ Weigthed average of embeddings"
      ],
      "metadata": {
        "id": "Bw9SoYuiMzxK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Combining all we have done so far into a single method to calculate attention"
      ],
      "metadata": {
        "id": "zmYy30P1TGmj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def scaled_dot_product_attention(query, key, value):\n",
        "  dim_k = query.size()[-1]\n",
        "  attn_scores = torch.bmm(query, key.transpose(1, 2))/sqrt(dim_k)\n",
        "  attn_wts = F.softmax(attn_scores, dim=1)\n",
        "  return torch.bmm(attn_wts, value)"
      ],
      "metadata": {
        "id": "65QtynU3TEiX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In practice, the query key and value are linear projections of the input embeddings to any layer (learnable and used to capture semantic relationships). Since these are different heads, the mechanism is called multi-headed attention. One head softmax focuses on one aspect of relationship. Several heads => several aspects (which are learned and not hand-engineered, similar to filters in CNNs) captured ex: subject-verb interaction, nearby adjective etc."
      ],
      "metadata": {
        "id": "r_1yy3rgUfBX"
      }
    }
  ]
}